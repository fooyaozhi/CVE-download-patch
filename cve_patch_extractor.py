from time import localtime, strftime
import multiprocessing
import requests
import urllib
import hashlib
import sys
from bs4 import BeautifulSoup as bs
import os
import re

from lib.Config import Configuration
from lib.DatabaseLayer import PostgresDb
from lib.Logging import Logger

logging_util = Logger(log_to_console=False, log_to_file=True)


DEBUG = False
VERBOSE = False
CACHE_OVERWRITE = False
SEARCH_DEEP = 1

add_apatch = ["sourceware.org", "www.ghostscript.com"]
add_download = ["bugs.php.net"]
add_format_dif = ["bugs.quassel-irc.org", "trac.xiph.org", "svn.boost.org", "ssl.icu-project.org",
                  "bugs.icu-project.org", "trac.nginx.org", "trac.osgeo.org", "trac.transmissionbt.com"]
add_format_txt = [".googlesource.com"]
add_raw = ["bitbucket.org", "patchwork.freedesktop.org"]
advisory = ["stable-channel-update", "/bulletin/", "/Releases/", "www.kde.org/info/security/", "blogs.gentoo.org", "secunia.com/advisories/", "googlechromereleases.blogspot.com", "oval.cisecurity.org/repository/search/definition/", "www.trustwave.com/spiderlabs/advisories/", "seclists.org/fulldisclosure/", "slackware.com/security/viewer.php?",
            "/www.securitytracker.com/id?", "www.vmware.com/security/advisories/", "security.netapp.com/advisory/", "lists.apple.com", "/security-advisory/", "support.apple.com/", "www.securitytracker.com/id/", "access.redhat.com/errata/", "www.redhat.com/support/errata/", "rhn.redhat.com/errata/", "lists.opensuse.org/opensuse-updates/"]
attachment = ["lists.gnu.org", "www.spinics.net/lists/", "crbug.com", "marc.info", "www.freebsd.org/security/advisories/",
              "bugs.edge.launchpad.net", "lwn.net/Articles/", "krbdev.mit.edu/rt/Ticket/Display.html?id=", "/cgi-bin/bugreport.cgi?", "xenbits.xen.org/xsa"]
bazaar_launchpad = ["bazaar.launchpad.net"]
black_list = ["www2.hpe.com", "/www.vupen.com/english/advisories/", "lists.fedoraproject.org/pipermail/package-announce/", "exchange.xforce.ibmcloud.com/vulnerabilities/", "www.oracle.com/technetwork/", "wiki.mozilla.org/", ".pdf", "security.gentoo.org/glsa/", "ubuntu.com/usn/", "www.mandriva.com/security/advisories",
              "lists.opensuse.org/opensuse-security-announce/", "www.wireshark.org/security/", "www.debian.org/security/", "www.exploit-db.com", "debian-lts-announce", "www.securityfocus.com", "openwall.com/lists/oss-security/", "fedorahosted.org/logrotate/changeset/", "www.openwall.com", "cve.mitre.org", "bk1.ntp.org", "bk.ntp.org"]
bug_debian = ["bugs.debian.org"]
bugzilla = ["bugzilla", "show_bug.cgi"]
code_google = ["code.google"]
code_review = ["codereview.chromium.org"]
codewireshark = ["code.wireshark.org"]
correct = ["/+attachment/", "&view=patch", "src.chromium.org", "codereview.chromium.org/download/",
           "attachment.cgi?id=", "bugs.freedesktop.org", "downloads.asterisk.org", "nginx.org/download", "lists.xen.org"]
dead_server = ["wwws.clamav.net", "hg.nih.at", "hg.dovecot.org", "trac.imagemagick", "svn.gna.org", "cyrus.foundation",
               "chinstrap.ubuntu.com", "qt.gitorious.org", "developer.pidgin.im", "libjpeg-turbo.svn.sourceforge.net", "cvs.openssl.org"]
git_repo = ["/cgit/", "/gitweb.", ".git;", "/git/?p=",
            "//git.", "/gitweb/", ".git/commit/?id=", ".git/patch/?id="]
github = ["//github.com", "salsa.debian.org/debian/w3m/"]
gitlab = ["//gitlab.com", "//gitlab.gnome.org/", "//gitlab.freedesktop.org",'//git.gnome.org']
gitphp = ["git.php.net"]
ghostscript = ["git.ghostscript.com"]
graphicmagick = ["hg.graphicsmagick.org"]
icinga = ["dev.icinga.org"]
issues = ["/issues/"]
linus_repo = ["git.kernel.org/linus"]
mageia = ["advisories.mageia.org"]
mirror_repo = ["projects.kde.org", "commits.kde.org"]
pidgin = ["hg.pidgin.im"]
replace_commit_patch = ["anonscm.debian.org", "source.codeaurora.org", "cgit.", "gitorious.org",
                        "anongit.mindrot.org", "perl5.git.", "repo.", "git.kernel.org/cgit", "git.kernel.org/pub", "cgit.freedesktop.org"]
replace_ref_dif = ["bazaar.", "bzr."]
replace_rev_raw = ["hg.openjdk.java.net", "www.sudo.ws", "hg.code.sf.net", "hg.nih.at", "hg.mozilla.org",
                   "secure.ucc.asn.au", "xenbits.xensource.com", "icedtea.classpath.org", "hg.python.org"]
swi_prolog = ["www.swi-prolog.org"]
viewvc = ["svn.apache.org", "svnweb.freebsd.org", "svn.linuxsampler.org", "svn.apache.org", "websvn.kde.org",
          "svn.php.net", "src.chromium.org", "vcs.pcre.org", "gcc.gnu.org", "svn.ruby-lang.org", "codereview.qt-project.org"]
web_mit_edu = ["web.mit.edu"]
webkit = ["trac.webkit.org"]
wireshark = ["anonsvn.wireshark.org"]

runPath = os.path.dirname(os.path.realpath(__file__))

def get_Domain(url, internal_link):
    protocol = url.split("//")[0] + "//"
    link = url.split("//")[1]
    if internal_link[0] != "/":
        link = link[:link.rfind("/")] + "/"
    else:
        link = link.split("/")[0]
    return protocol + link


def do_correct(url, cve_id):
    output = do_pre_check(url, cve_id)
    if output:
        return output
    return url

def do_pre_check(url, cve_id): ####
    '''
    For each pre element in url provided, look for patch.
    If patch is not found, look for urls in pre and make sure cve_id is found in the page.
    '''
    pre_list = [str(pre) for pre in find_pre(url)]
    patch_list = []
    if pre_list:
        for pre in pre_list:
            pre = pre.lstrip('<pre>')
            pre = pre.rstrip('</pre>')
            if ('+++' and '---' and '@@' ) in pre: # Check if the pre contains patch 
                pre_hash = md5_hash(pre)
                with open(f'{LOG_DIR}/pre_patch_files/{pre_hash}','w') as f:
                    f.write(pre)
                patch_list.append('PREPATCH' + pre_hash) 
            else:
                ##TODO: check for urls in pre
                # url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
                url_pattern = re.compile(r'(?:http|ftp|https):\/\/(?:[\w_-]+(?:(?:\.[\w_-]+)+))(?:[\w.,@?^=%&:\/~+#-]*[\w@?^=%&\/~+#-])')
                urls = re.findall(url_pattern, pre)
                if urls:
                    for inner_url in set(urls):
                        # check if cve_id found in page
                        cont = download_HTML(inner_url)
                        if cve_id in str(cont):
                            if DEBUG:
                                print(f'Performing do_pre_check on {url}, \n found {inner_url}')
                            patch_list.append(inner_url)
    if patch_list == []:
        return url
    return patch_list


def md5_hash(data): ####
    '''
    Get hash value of the given data
    '''
    m = hashlib.md5()
    m.update(data.encode())
    return m.hexdigest()


def find_pre(url): ####
    '''
    Return list of <pre> elements in html of the given url
    '''
    response = requests.get(url)
    soup = bs(response.text, 'html.parser')
    pre_elements = soup.find_all('pre')
    return pre_elements 


def do_github(url):
    url = url.replace("https://github.com//login?return_to=", "")
    if url.endswith(".patch") or url.endswith(".diff"):
        return url
    if url.endswith("master") or url.endswith("commits") or "/blob/" in url:
        return ""
    if "/commit/" not in url:
        return ""
    url = url[:url.rfind("?")]
    url = url[:url.rfind("#")]
    return url + ".patch"


def do_gitlab(url):
    gitlab_list = ["//gitlab.com", "//gitlab.gnome.org/", "//gitlab.freedesktop.org",'//git.gnome.org']
    if url.endswith("master") or url.endswith("commits") or "/blob/" in url:
        return ""
    if "/commit/" not in url:
        return ""
    for key_word in gitlab_list:
        if key_word in url:
            return url + ".patch"
    return url

def do_ghostscript(url):
    if 'commitdiff' in url:
        return url.replace('commitdiff', 'commitdiff_plain')
    # old logic
    return url + ";a=patch"

def do_add_download(url):
    return url + "&download=1"


def do_bazaar_launchpad(url):
    if "/revision/" in url:
        return url.replace("/revision/", "/diff/")
    return ""


def do_add_apatch(url):
    if "findgit.cgi?" in url:
        url = get_redirect_link(url)
    if isinstance(url, (bytes, bytearray)):
        url = url.decode('utf-8')
    return url + ";a=patch"


def do_add_raw(url):
    return url + "/raw"


def do_linus_repo(url):
    return url.replace('//git.kernel.org/linus/', '//git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/patch/?id=')


def do_replace_commit_patch(url):
    url = url.replace("commit", "patch")
    url = url.replace("patchdiff", "patch")
    return url


def do_replace_rev_raw(url):
    return url.replace("rev", "raw-rev")


def do_add_format_txt(url):
    reg = re.compile("(.*[0-9a-f]{40})")
    commit_id = reg.findall(url)
    if commit_id != []:
        return commit_id[0] + "^!/?format=TEXT"
    return ""


def do_add_format_dif(url):
    return url + "?format=diff"


def do_replace_ref_dif(url):
    return url.replace("revision", "diff")


def do_dead_server(url):
    if "qt.gitorious.org" in url:
        return url.replace("qt.gitorious.org", "gitorious.org").replace("www.gitorious.org", "gitorious.org").replace("commit", "patch")
    else:
        return ""


def do_swi_prolog(url):
    url = url.replace("www.swi-prolog.org/git/packages/xpce.git",
                      "github.com/SWI-Prolog/packages-xpce")
    url = url.replace("commitdiff", "commit")
    url += ".patch"
    return url


def do_icinga(url):
    return url.replace("dev.icinga.org/projects/icinga-core/repository/revisions", "github.com/Icinga/icinga-core/commit") + ".patch"


def do_pidgin(url):
    return url.replace("hg.pidgin.im/pidgin/main/rev/", "bitbucket.org/pidgin/main/commits/") + "/raw"


def do_gitphp(url):
    return url.replace("commit", "commitdiff_plain")


def do_webkit(url):
    return url + "/webkit?format=diff"


def do_graphicmagick(url):
    url = url.replace("?cmd=changeset;node=", "/raw-rev/")
    url = url.replace("/rev/", "/raw-rev/")
    return url


def do_codewireshark(url):
    # need to be improve
    if "/review/#/q/commit:" in url:
        return url

    return do_replace_commit_patch(url)


def do_add_unhandle(url):
    if url == "":
        return url
    return "UNHANDLE: " + url


def do_git_repo(url):
    if 'git.gnome.org' in url:
        return do_gitlab(url)
    if "/shortlog/" in url:
        url = url.replace("/shortlog/", "/patch/")
    if "/log/" in url:
        url = url.replace("/log/", "/patch/")
    if "/tree/" in url:
        url = url.replace("/tree/", "/patch/")
    if "/commitdiff/" in url:
        url = url.replace("/commitdiff/", "/patch/")
    if "/commit/" in url:
        url = url.replace("/commit/", "/patch/")
    if "/commit?" in url and "https://git.kernel.org/" in url:
        url = url.replace("/commit?", "/patch?")
    if "a=commit;" in url:
        url = url.replace("a=commit;", "a=patch;")
    if "a=commitdiff;" in url:
        url = url.replace("a=commitdiff;", "a=patch;")
    if "/blob/rpms!" in url:
        url = url.replace("/blob/rpms!", "/raw/rpms/")
    if "sourceware.org" in url:
        url = do_add_apatch(url)
    if "git.clamav.net/" in url:
        reg = re.compile("([0-9a-f]{40})")
        commit_id = reg.findall(url)
        if commit_id != []:
            url = "https://github.com/clamwin/clamav/commit/%s.patch" % commit_id[0]
        else:
            url = ""
    if "torproject.org" in url:
        reg = re.compile("([0-9a-f]{40})")
        commit_id = reg.findall(url)
        if commit_id != []:
            url = "https://oniongit.eu/nickm/tor/commit/%s.patch" % commit_id[0]
    if ";h=" in url and "a=" not in url:
        url = do_add_apatch(url)
    if "a=summary" in url or "a=shortlog" in url or "a=log" in url or "a=search_help" in url:
        url = ""
    return url


def do_bug_debian(url):
    url = url.replace("bugs.debian.org/bugreport.cgi",
                      "bugs.debian.org/cgi-bin/bugreport.cgi")
    return url


def do_mirror_repo(url):
    if "commits.kde.org" in url:
        reg = re.compile("commits\.kde\.org\/([^\/]*)\/")
        prj_name = reg.findall(url)
        if prj_name != []:
            prj_name = prj_name[0]
        else:
            return ""
        url = url.replace("commits.kde.org/", "cgit.kde.org/")
        url = url.replace("/%s/" % prj_name, "/%s.git/patch/?id=" % prj_name)
    if "projects.kde.org" in url:
        reg = re.compile("\/kde\/([^\/]*)\/repository")
        prj_name = reg.findall(url)
        if prj_name != []:
            prj_name = prj_name[0]
        else:
            return ""
        url = url.replace("projects.kde.org/projects/kde/", "cgit.kde.org/")
        url = url.replace("/%s/repository/revisions/" %
                          prj_name, "/%s.git/patch/?id=" % prj_name)

    return url


# Search for direct link


def do_viewvc(url):
    if "view=patch" in url:
        return [url]
    url = url.replace("http://vcs.pcre.org", "https://vcs.pcre.org")
    cnt = download_HTML(url)
    if isinstance(cnt, (bytes, bytearray)):
        cnt = cnt.decode('utf-8')

    result = []
    reg = re.compile('<a href="([^"]*)" title="View Diff">text changed')
    for f in reg.findall(cnt):
        if ('.c' in f) or ('.cpp' in f) or ('.h' in f) or ('.cxx' in f):
            url = get_Domain(url, f) + f + '&view=patch'
            result.append(url)

    if result != []:
        return result
    return 0


def do_web_mit_edu(url):
    if "patch" not in url:
        change_log = download_HTML(url)
        if isinstance(change_log, (bytes, bytearray)):
            change_log = change_log.decode('utf-8')
        reg = re.compile(
            'http\:\/\/web\.mit\.edu\/kerberos\/advisories\/(?!asc).*txt')
        return list(set(reg.findall(change_log)))
    else:
        return url


def do_issues(url):
    if "github.com" not in url:
        if "code.google.com" in url:
            return do_code_google(url)
        return 0

    reg = re.compile('<a href="([^"]*)" class="commit-id">')
    cnt = download_HTML(url)
    if isinstance(cnt, (bytes, bytearray)):
        cnt = cnt.decode('utf-8')
    commit_urls = reg.findall(cnt)

    result = list(map(lambda url: 'https://github.com' + url, commit_urls))
    if result == []:
        return 0
    return result


def do_wireshark(url):
    reg = re.compile("[pathrev|revision]=([0-9]*)")
    searchLink = "https://code.wireshark.org/review/gitweb?p=wireshark.git&a=search&h=HEAD&st=commit&s=revision=" + \
        reg.findall(url)[0] + "$&sr=1"
    tmp = search_Patch_Link(searchLink)
    result = []
    if tmp != 0:
        for j in tmp:
            if "h=HEAD" not in j and "commitdiff" in j:
                result.append(
                    j.replace("commitdiff", "commit").replace("commit", "patch"))
    result = list(set(result))

    if result != []:
        return result
    return 0


def do_bugzilla(url):
    if "attachment.cgi?" in url:
        return [url]
    # Create new instance of BugZilla object with cve_id = N/A, is_url = 1, url = i
    tmp = BugZilla("N/A", 1, url)
    tmp.search_attachments()
    tmp.search_direct_links()

    result = tmp.clean()
    if result != []:
        return result
    return 0


def do_code_google(url):
    cnt = download_HTML(url)
    if isinstance(cnt, (bytes, bytearray)):
        cnt = cnt.decode('utf-8')
    reg = re.compile('<a href="([^"]*)"')
    tmp = reg.findall(cnt)

    result = []
    for j in tmp:
        if "src.git" in j:
            y = do_add_format_txt(j)
            if y != "":
                result.append(y)
        if "viewvc" in j and "view=rev" in j:
            y = do_viewvc(j)
            if y != 0:
                result += y
    if result != []:
        return list(set(result))
    else:
        return 0


def do_code_review(url):
    l = search_Patch_Link(url)
    if l == 0:
        return 0
    result = []
    for i in l:
        if ".diff" in i:
            result.append(i)
    if result == []:
        return 0
    return result


def do_mageia(url):
    cnt = download_HTML(url)
    if isinstance(cnt, (bytes, bytearray)):
        cnt = cnt.decode('utf-8')
    reg = re.compile('<a href="([^"]*)"')
    r = reg.findall(cnt)

    keyword = ["bugzilla", "show_bug.cgi"]
    result = []
    for i in r:
        for j in keyword:
            if j in i:
                if "://" not in i:
                    i = get_Domain(url, i) + i
                result.append(i)

    if result == []:
        return 0

    final = []
    for i in result:
        tmp = do_bugzilla(i)
        if tmp != 0:
            final += tmp

    if final != []:
        return final
    return 0


def do_attachment(url):
    if url.endswith(".patch") or url.endswith(".diff") or url.endswith(".txt") or url.endswith(".c") or url.endswith(".cpp") or url.endswith(".h"):
        return [url]
    return search_Patch_Link(url)


def do_search_direct_link(url):
    return search_Patch_Link(url)


def do_black_list(url):
    # To be implement
    return []


def do_advisory(url):
    # To be implement
    return 0


exten_dict = {
    "viewvc": [viewvc, do_viewvc],
    "web_mit_edu": [web_mit_edu, do_web_mit_edu],
    "issues": [issues, do_issues],
    "wireshark": [wireshark, do_wireshark],
    "bugzilla": [bugzilla, do_bugzilla],
    "code_google": [code_google, do_code_google],
    "code_review": [code_review, do_code_review],
    "mageia": [mageia, do_mageia],
    "attachment": [attachment, do_attachment],
    "black_list": [black_list, do_black_list],
    "advisory": [advisory, do_advisory],
}
replace_dict = {
    "add_apatch": [add_apatch, do_add_apatch],
    "add_download": [add_download, do_add_download],
    "add_format_dif": [add_format_dif, do_add_format_dif],
    "add_format_txt": [add_format_txt, do_add_format_txt],
    "add_raw": [add_raw, do_add_raw],
    "advisory": [advisory, do_advisory],
    "bug_debian": [bug_debian, do_bug_debian],
    "bazaar_launchpad": [bazaar_launchpad, do_bazaar_launchpad],
    "codewireshark": [codewireshark, do_codewireshark],
    "correct": [correct, do_correct],
    "dead_server": [dead_server, do_dead_server],
    "linus_repo": [linus_repo, do_linus_repo],
    "git_repo": [git_repo, do_git_repo],
    "github": [github, do_github],
    "gitlab": [gitlab, do_gitlab],
    "gitphp": [gitphp, do_gitphp],
    "ghostscript": [ghostscript, do_ghostscript],
    "graphicmagick": [graphicmagick, do_graphicmagick],
    "icinga": [icinga, do_icinga],
    "mirror_repo": [mirror_repo, do_mirror_repo],
    "pidgin": [pidgin, do_pidgin],
    "replace_commit_patch": [replace_commit_patch, do_replace_commit_patch],
    "replace_ref_dif": [replace_ref_dif, do_replace_ref_dif],
    "replace_rev_raw": [replace_rev_raw, do_replace_rev_raw],
    "swi_prolog": [swi_prolog, do_swi_prolog],
    "webkit": [webkit, do_webkit],
}

def log_patches_found(cve_id, urls, url_confidence, file_log=1):
    module_name = __name__ + '.' + log_patches_found.__name__

    print(cve_id + ", ".join(urls))
    if urls != 0:
        for i in urls:
            try:
                out_file = LOG_FILE_NAME
                confidence_level = -1
                if i in url_confidence:
                    confidence_level = url_confidence[i]

                if "PREPATCH" in i:
                    i = i.replace("UNHANDLE: ", "")
                elif "UNHANDLE" in i:
                    i = i.replace("UNHANDLE: ", "")
                    out_file = "UNHANDLE_" + out_file
                out_file = os.path.join(LOG_DIR, out_file)

                log_message = "%s\t%s\t%s" % (cve_id.replace(".html", ""), i, confidence_level)
                if file_log or VERBOSE:
                    open(out_file, "a").write(log_message + "\n")
                    if VERBOSE:
                        print(log_message)
                else:
                    pass # print(log_message)

            except Exception as e:
                logging_util.log(module_name, (cve_id + i + str(e)), level='error')


def md5_hash(data):
    m = hashlib.md5()
    m.update(data.encode())
    return m.hexdigest()


def download_HTML(url, overwrite=CACHE_OVERWRITE):
    module_name = __name__ + '.' + download_HTML.__name__
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'}
    try:
        cnt = ""

        cache_dir = os.path.join(runPath, 'cache')
        if not os.path.exists(cache_dir):
            os.mkdir(cache_dir)
        
        cache_file = os.path.join(cache_dir, md5_hash(url))

        if not os.path.exists(cache_file) or overwrite is True:
            response = requests.get(url, headers=headers, timeout=120)
            if "text" in response.headers['content-type']:
                cnt = response.text.encode("utf8")
                open(cache_file, "wb").write(cnt)
        else:
            cnt = open(cache_file, "rb").read()
    except Exception as e:
        if DEBUG:
            logging_util.log(module_name, "Can not download:" + url + str(e), level='error')
        cnt = ""
    return cnt


def get_redirect_link(url):
    module_name = __name__ + '.' + get_redirect_link.__name__
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'}
    try:
        response = requests.get(url, headers=headers, timeout=120)
        return response.url.encode("utf8")
    except Exception as e:
        logging_util.log(module_name, "Can not download:" + url + str(e), level='error')
        return url

# Dead pronounced by Paul
def crawl_canonical(cve_id):
    url = "https://people.canonical.com/~ubuntu-security/cve/%s/%s.html" % (
        cve_id[4:8], cve_id)
    content = download_HTML(url)
    if isinstance(content, (bytes, bytearray)):
        content = content.decode('utf-8')
    return content


def crawl_NVD(cve_id):
    url = "https://nvd.nist.gov/vuln/detail/%s" % cve_id
    content = download_HTML(url)
    if isinstance(content, (bytes, bytearray)):
        content = content.decode('utf-8')
    return content


def crawl_Debian_Bug_Tracker(cve_id):
    url = "https://security-tracker.debian.org/tracker/%s" % cve_id
    content = download_HTML(url)
    if isinstance(content, (bytes, bytearray)):
        content = content.decode('utf-8')
    return content


def crawl_RedHat_BugZilla(cve_id):
    url = "https://bugzilla.redhat.com/show_bug.cgi?id=%s" % cve_id
    content = download_HTML(url)
    if isinstance(content, (bytes, bytearray)):
        content = content.decode('utf-8')
    return content

# Dead pronounced by Paul
def crawl_CodeAurora(cve_id):
    url = "https://www.codeaurora.org/security-advisories?search=%s" % cve_id
    cnt = download_HTML(url)
    if isinstance(cnt, (bytes, bytearray)):
        cnt = cnt.decode('utf-8')

    result = []
    reg = re.compile('<a href="([^"]*)">')
    header = find_substring('<div class="security-advisory-list">', cnt)

    if len(header) == 0:
        return ""

    for p in header:
        t = cnt[p:]

        table_1 = t.find('<h5>')
        table_2 = t.find('</h5>')

        t = cnt[p + table_1: p + table_2]
        tmp = reg.findall(t)
        if tmp == []:
            continue
        else:
            result += tmp

    if result == []:
        return ""

    cnts = ""
    for i in result:
        downloaded_content = download_HTML(i)
        if isinstance(downloaded_content, (bytes, bytearray)):
            downloaded_content = downloaded_content.decode('utf-8')
        cnts += downloaded_content

    return cnts


def check_Type(url, list):
    for i in list:
        if i in url:
            return 1
    return 0


def find_substring(substring, string):
    """
    Returns list of indices where substring begins in string

    >>> find_substring('me', "The cat says meow, meow")
    [13, 19]
    """
    indices = []
    index = -1  # Begin at -1 so index + 1 is 0
    while True:
        #  Find next index of substring, by starting search from index + 1
        index = string.find(substring, index + 1)
        if index == -1:
            break  # All occurrences have been found
        indices.append(index)
    return indices


def search_Patch_Link(url, isURL=1, cnt=""):
    if isURL:
        cnt = download_HTML(url)
        if isinstance(cnt, (bytes, bytearray)):
            cnt = cnt.decode('utf-8')
    # new logic
    has_fix = 0
    introduce, info_list, result = [], [], []
    reg_1 = re.compile('(https?\:[^" \n\<\>\n\\\]*)')

    soup = bs(cnt, "html.parser")
    table = soup.find("pre")
    if table:
        for br in table.find_all('br'):
            br.replace_with("\n")
        info = table.getText()
        info_list = info.split('\n')

    if info_list:
        for line in info_list:
            if 'fixed by:' in line.lower():
                has_fix = 1
                result.extend(reg_1.findall(line))
            if 'introduced by:' in line.lower():
                introduce.extend(reg_1.findall(line))

    if not has_fix:
        # use old logic
        reg_2 = re.compile('<a href="([^"]*)"')
        reg_3 = re.compile('(https?\:[^" \n\<\>\n\\\]*)')
        r = reg_2.findall(cnt)
        r += reg_3.findall(cnt)

        keyword = ["patch", "diff", ".txt", "commit", "revision", ".git", "git."]
        bad = ["www.exploit-db.com", "mailto"]

        for i in r:
            for j in keyword:
                if j in i:
                    is_bad = 0
                    for x in bad:
                        if x in i:
                            is_bad = 1
                    if is_bad == 1:
                        continue
                    if "://" not in i:
                        i = get_Domain(url, i) + i
                    result.append(i)
    result = list(set(result) - set(introduce))
    # old logic
    # reg = re.compile('<a href="([^"]*)"')
    # reg_2 = re.compile('(https?\:[^" \n\<\>\n\\\]*)')
    # r = reg.findall(cnt)
    # r += reg_2.findall(cnt)
    #
    # keyword = ["patch", "diff", ".txt", "commit", "revision", ".git", "git."]
    # bad = ["www.exploit-db.com", "mailto"]
    #
    # result = []
    # for i in r:
    #     for j in keyword:
    #         if j in i:
    #             is_bad = 0
    #             for x in bad:
    #                 if x in i:
    #                     is_bad = 1
    #             if is_bad == 1:
    #                 continue
    #             if "://" not in i:
    #                 i = get_Domain(url, i) + i
    #             result.append(i)

    if result != []:
        return result

    return 0


class Canonical:
    def __init__(self, cve_id):
        self.id = cve_id
        self.cnt = crawl_canonical(cve_id)
        self.patch_links = []

    def search_fixed_by(self):
        reg = re.compile('Fixed by <a href="([^"]*)">')
        urls = reg.findall(self.cnt)

        if urls:
            self.patch_links += urls
            return 1
        return 0

    def search_patch_table(self):
        reg = re.compile('(?!Introduced\ by).*<a href="([^"]*)"')
        header = find_substring(
            '<div class="patches">Patches:</div>', self.cnt)
        result = []

        if len(header) == 0:
            return 0

        for p in header:
            t = self.cnt[p:]

            table_1 = t.find('<table>')
            table_2 = t.find('</table>')

            t = self.cnt[p + table_1: p + table_2]
            urls = reg.findall(t)
            if urls:
                result += urls

        if result != []:
            self.patch_links += result
            return 1
        return 0

    def search_references(self):
        reg = re.compile('<a href="([^"]*)">')
        header = find_substring(
            '<div class="field">References</div>', self.cnt)
        result = []

        if len(header) == 0:
            return 0

        for p in header:
            t = self.cnt[p:]

            table_1 = t.find('<div')
            table_2 = t.find('\n')

            t = self.cnt[p + table_1: p + table_2]
            urls = reg.findall(t)
            if urls:
                result += urls

        if result != []:
            self.patch_links += result
            return 1
        return 0

    def search_bugs_table(self):
        reg = re.compile('<a href="([^"]*)">')
        header = find_substring('<div class="field">Bugs</div>', self.cnt)
        result = []

        if len(header) == 0:
            return 0

        for p in header:
            t = self.cnt[p:]

            table_1 = t.find('<div')
            table_2 = t.find('\n')

            t = self.cnt[p + table_1: p + table_2]
            urls = reg.findall(t)
            if urls:
                result += urls

        if result != []:
            self.patch_links += result
            return 1
        return 0

    def clean(self):
        self.patch_links = list(set(self.patch_links))
        if DEBUG:
            print("Canonical")
            for i in self.patch_links:
                print("    ", i)
        return self.patch_links


class NVD:
    def __init__(self, cve_id):
        self.id = cve_id
        self.cnt = crawl_NVD(cve_id)
        self.patch_links = []
        self.project_name = "N/A"

    def search_vuln_references(self):
        soup = bs(self.cnt, "html.parser")
        urls = [tag.text for tag in soup.findAll(attrs={"data-testid": re.compile("^vuln-hyperlinks-link-")})]
        if urls:
            self.patch_links += urls
            return 1
        return 0

    def get_project_name(self):
        reg = re.compile('cpe:2\.3:a:([^:]*):([^:]*):')
        n = reg.findall(self.cnt)
        if n:
            self.project_name = n[0][0]
        else:
            self.project_name = "Not Found"
        return self.project_name

    def clean(self):
        self.patch_links = list(set(self.patch_links))
        if DEBUG:
            print("NVD")
            for i in self.patch_links:
                print("    ", i)
        return self.patch_links


class BugZilla:
    def __init__(self, cve_id, is_url=0, url=""):
        if is_url == 0:
            self.id = cve_id
            self.cnt = crawl_RedHat_BugZilla(cve_id)
            self.patch_links = []
            self.url = "https://bugzilla.redhat.com/show_bug.cgi?id=%s" % self.id
        else:
            self.id = "N/A"
            self.cnt = download_HTML(url)
            if isinstance(self.cnt, (bytes, bytearray)):
                self.cnt = self.cnt.decode('utf-8')
            self.patch_links = []
            self.url = url

    def search_attachments(self):
        module_name = __name__ + '.' + self.search_attachments.__name__

        result = []

        if "attachment.cgi" not in self.url:
            reg = re.compile('<a href="(attachment.cgi\?id=[0-9]*)"')
            for l in reg.findall(self.cnt):
                try:
                    result.append(
                        self.url[:self.url.index("show_bug.cgi?")] + l)
                except Exception as e:
                    logging_util.log(module_name, self.id + self.url + str(e), level='error')
        else:
            result = [self.url]

        if result != []:
            for i in result:
                tmp = i.replace("action=edit", "action=view")
                tmp = tmp.replace("action=diff", "action=view")
                self.patch_links.append(tmp)
            return 1
        return 0

    def search_direct_links(self):
        urls = search_Patch_Link(self.url, 0, self.cnt)
        if urls != 0:
            self.patch_links += urls

    def clean(self):
        self.patch_links = list(set(self.patch_links))
        if DEBUG:
            print("BugZilla")
            for i in self.patch_links:
                print("    ", i)
        return self.patch_links


class Debian:
    def __init__(self, cve_id):
        self.id = cve_id
        self.cnt = crawl_Debian_Bug_Tracker(cve_id)
        self.patch_links = []
        self.url = "https://security-tracker.debian.org/tracker/%s" % cve_id

    def search_direct_links(self):
        urls = search_Patch_Link(self.url, 0, self.cnt)
        if urls != 0:
            self.patch_links += urls

    def clean(self):
        self.patch_links = list(set(self.patch_links))
        if DEBUG:
            print("Debian")
            for i in self.patch_links:
                print("    ", i)
        return self.patch_links


class CodeAurora:
    def __init__(self, cve_id):
        self.id = cve_id
        self.cnt = crawl_CodeAurora(cve_id)
        self.patch_links = []
        self.base_url = "https://www.codeaurora.org/security-advisory"

    def search_direct_links(self):
        urls = search_Patch_Link(self.base_url, 0, self.cnt)
        if urls != 0:
            self.patch_links += urls

    def clean(self):
        self.patch_links = list(set(self.patch_links))
        if DEBUG:
            print("CodeAurora")
            for i in self.patch_links:
                print("    ", i)
        return self.patch_links


class PatchUrlsProcess:
    #patch link confidence: The higher the value, the lower the probability that it is a good patch.
    RAW = 0
    REFER_LINK = 1
    SECOND_LINK = 2
    THIRD_LINK = 3

    def __init__(self, url_list, cve_id):
        self.urls = list(set(url_list))
        self.result = []
        self.cve_id = cve_id
        self.init_confidence()

    def init_confidence(self):
        self.url_confidence = {}
        for url in self.urls:
            self.url_confidence[url] = self.RAW

    def update_confidence(self, url, confidence_level):
        if url in self.url_confidence:
            pass
        else:
            self.url_confidence[url] = confidence_level

    def show_confidence(self):
        for k,v in self.url_confidence.items():
            print(str(v) + " : " + k)

    def get_confidence(self):
        return self.url_confidence

    def process(self):
        # find as much as possible direct links that relate to it's CVE
        for url in self.urls:
            i = standardlize_URL(url)
            self.update_confidence(i, self.REFER_LINK)
            func, type = get_type(i, exten_dict, 1)

            if DEBUG:
                print("EXTEND:    ", type, i)

            if func == do_correct:
                tmp = func(i, self.cve_id)
            else:
                tmp = func(i)
            if tmp != 0:
                if not isinstance(tmp, list):
                    if DEBUG:
                        print("    ", tmp)
                    tmp_url = standardlize_URL(tmp)
                    self.result.append(tmp_url)
                    self.update_confidence(tmp_url, self.SECOND_LINK)
                else:
                    for j in tmp:
                        if DEBUG:
                            print("    ", j)
                        tmp_url = standardlize_URL(j)
                        self.result.append(tmp_url)
                        self.update_confidence(tmp_url, self.SECOND_LINK)

        self.result = list(set(self.result))
        # Deep search
        for i in range(SEARCH_DEEP):
            tmp_result = list(self.result)
            for j in tmp_result:
                tmp_url = standardlize_URL(j)
                func, type = get_type(tmp_url, exten_dict, 1)

                if type != "UNHANDLE EXTEND":
                    if DEBUG:
                        print("EXTEND:    ", type, tmp_url)
                    if func == do_correct:
                        tmp = func(tmp_url, self.cve_id)
                    else:
                        tmp = func(tmp_url)
                    if tmp != 0:
                        if not isinstance(tmp, list):
                            if DEBUG:
                                print("    ", tmp)
                            self.result.append(tmp)
                            self.update_confidence(tmp, self.THIRD_LINK)
                            self.result.remove(tmp_url)
                        else:
                            if DEBUG:
                                for k in tmp:
                                    print("    ", k)
                            self.result.remove(tmp_url)
                            self.result += tmp
                            for url_tmp in tmp:
                                self.update_confidence(url_tmp, self.THIRD_LINK)

        # convert found commit links to patch links
        for i in range(len(self.result)):
            confidence_level = -1
            if self.result[i] in self.url_confidence:
                confidence_level = self.url_confidence[self.result[i]]
            tmp = standardlize_URL(self.result[i])
            func, type = get_type(tmp, replace_dict, 0)

            if DEBUG:
                print("REPLACE:    ", type, tmp)

            if func == do_correct:
                tmp = func(tmp, self.cve_id)
            else:
                tmp = func(tmp)
            if tmp == 0:
                tmp = ""
            self.update_confidence(tmp, confidence_level)
            self.result[i] = tmp

    def get(self):
        try:
            self.result = list(set(self.result))
            self.result.remove("")
        except Exception:
            pass
        return self.result


def standardlize_URL(url):
    module_name = __name__ + '.' + standardlize_URL.__name__
    url = url.split(" ")[0]
    tmp = url
    # Some encoding bugs happen here
    try:
        tmp = urllib.parse.unquote(tmp)
    except Exception as e:
        logging_util.log(module_name, tmp[:30] + str(e), level='error')

    bad_surfix = ["/", "?"]
    for j in bad_surfix:
        if tmp != "" and tmp[-1] == j:
            tmp = tmp[:-1]
    tmp = tmp.replace(":80/", "/")

    return tmp


def get_type(url, dict, is_extend=1):
    for type in dict:
        for entry in dict[type][0]:
            if entry in url:
                return dict[type][1], type

    if is_extend == 1:
        return do_correct, "UNHANDLE EXTEND"
    if url.endswith(".patch") or url.endswith(".diff") or url.endswith(".txt"):
        return do_correct, "Correct"
    return do_add_unhandle, "UNHANDLE REPLACE"


def process_on_Canonical(cve_id, lock, file_log=1):
    obj = Canonical(cve_id)
    obj.search_fixed_by()
    obj.search_patch_table()
    obj.search_references()
    obj.search_bugs_table()

    return obj.clean()


def process_on_NVD(cve_id, lock, file_log=1):
    obj = NVD(cve_id)
    obj.search_vuln_references()

    return obj.clean()


def process_on_BugZilla(cve_id, lock, file_log=1):
    obj = BugZilla(cve_id)
    obj.search_attachments()
    obj.search_direct_links()

    return obj.clean()


def process_on_Debian(cve_id, lock, file_log=1):
    obj = Debian(cve_id)
    obj.search_direct_links()

    return obj.clean()


def process_on_CodeAurora(cve_id, lock, file_log=1):
    obj = CodeAurora(cve_id)
    obj.search_direct_links()

    return obj.clean()


def get_project_name(cve_id, lock):
    module_name = __name__ + '.' + get_project_name.__name__
    obj = NVD(cve_id)
    n = obj.get_project_name()

    lock.acquire()
    logging_util.log(module_name, "%s    %s" % (cve_id, n))
    lock.release()


def worker(cve_queue, lock, log_file):
    module_name = __name__ + '.' + worker.__name__
    while 1:
        cve_id = cve_queue.get()
        if cve_id == "STOP":
            break

        result = []
        # get_project_name(cve_id, lock)
        print ("[Processor %d] processing %s" % (os.getpid(), cve_id))

        result += process_on_NVD(cve_id, lock, log_file)
        result += process_on_BugZilla(cve_id, lock, log_file)
        result += process_on_Debian(cve_id, lock, log_file)
        # Dead pronounced by Paul
        # result += process_on_CodeAurora(cve_id, lock, log_file)
        # result += process_on_Canonical(cve_id, lock, log_file)

        result = list(set(result))
        # print("RESULT " + ", ".join(result))
        patches = PatchUrlsProcess(result, cve_id)
        patches.process()
        url_condifence = patches.get_confidence()

        try:
            lock.acquire()
            log_patches_found(cve_id, patches.get(), url_condifence, log_file)
        except Exception as e:
            logging_util.log(module_name, cve_id + str(e), level='error')
        finally:
            lock.release()
    return 1


LOG_DIR = os.path.join(runPath, 'Log')
LOG_FILE_NAME = "FOUND_%s" % strftime("%Y-%m-%d_%H-%M", localtime())
PRE_PATCH_DIR = os.path.join(LOG_DIR, 'pre_patch_files')

DEBUG=1
VERBOSE=1

if __name__ == "__main__":
    module_name = __name__ + '.' + '__main__'

    # init Log dir
    if not os.path.exists(LOG_DIR):
        os.mkdir(LOG_DIR)
    if not os.path.exists(PRE_PATCH_DIR):
        os.mkdir(PRE_PATCH_DIR)
    open(os.path.join(LOG_DIR, LOG_FILE_NAME), "a").write("")
    open(os.path.join(LOG_DIR, 'History'), "a").write(LOG_FILE_NAME + "\n")
    
    
    try: 
        if len(sys.argv) > 1 and os.path.exists(sys.argv[1]):
            list_file = sys.argv[1]
            cve_list = open(list_file, "r").read().splitlines()
            cve_list = list(map(lambda x: x.strip(), cve_list))
            cve_list = list(set(cve_list))
            if "" in cve_list:
                cve_list.remove("")
        # else:
        #     # Read CVE list from the postgres db
        #     postgresdb = PostgresDb(*Configuration.getPostgresConnection())
        #     cves_unprocessed = postgresdb.get_cves_unprocessed()
        #     cve_list = cves_unprocessed.keys()
        print(f'Number of CVE(s) to process: {len(cve_list)}')
        # Temp code - write into finalResult so we know which CVEs are
        # being examined currently
        with open(os.path.join(LOG_DIR, 'finalResult'), 'w') as f:
            f.writelines("%s\n" % l for l in cve_list)
    except Exception:
        # print("Can not open file:", list_file)
        # logging_util.log(module_name, 'Cannot retrieve CVEs from postgres db', level='error')
        logging_util.log(module_name, 'Cannot retrieve CVEs to process patch', level='error')
        # print ("Cannot retrieve CVEs from postgres db")
        exit(0)

    # status variables
    log_file = 1
    process_num = multiprocessing.cpu_count() * 2
    cve_queue = multiprocessing.Queue()
    proces_list = []
    lock = multiprocessing.Lock()

    for i in cve_list:
        cve_queue.put(i)

    # spawn multiprocesses
    logging_util.log(module_name, "Begin patch collection ...")
    for i in range(process_num):
        p = multiprocessing.Process(
            target=worker, args=(cve_queue, lock, log_file, ))
        proces_list.append(p)
        cve_queue.put("STOP")
        p.start()

    for p in proces_list:
        p.join()

    print("Extract Done")
    logging_util.log(module_name, "Extract Done ...")