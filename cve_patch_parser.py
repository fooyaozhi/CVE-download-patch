from datetime import datetime
import hashlib
import json
import os
import re
from unidiff import PatchSet
from LanguageDetector import LanguageDetector
import itertools
from dateutil import parser

hund_counter = itertools.count(start=1, step=1)
patch_counter = itertools.count(start=1, step=1)
# PATCH_DIRECTORY = r"/Volumes/MacExpand/workspace/myCodeManager/scantist_code_test/g9_data_process/patch_files"
runPath = os.path.dirname(os.path.realpath(__file__))
PATCH_DIRECTORY = os.path.join(runPath, 'srj-patch_files', 'patch')

def strip_comments( code_block, language):
    module_name = __name__ + '.' + strip_comments.__name__

    regex_language_key = language
    # If language is java or js, use c style comment matching
    # If language is perl, use python comment matching
    try:
        if language in ['java', 'javascript']:
            regex_language_key = 'c'
        elif language == 'perl':
            regex_language_key = 'python'
        if regex_language_key in comments_regex:
            for pattern in comments_regex[regex_language_key]:
                code_block = re.sub(pattern, '', code_block)
        # If multiple empty lines, replace with one
        code_block = re.sub(r'\n+', '\n', code_block)
    except:
        print(module_name,
                        'Error during comment removal', level='error')
    return code_block



def get_md5_hash(content, strip_spacing=False):
    # Removing extra spacing in each line
    if strip_spacing:
        content = list(map(lambda x: x.strip(), content.split('\n')))
        content = ''.join(content)
    # Simply remove \n from the code
    else:
        content = content.replace('\n', '')
    m = hashlib.md5()
    m.update(content.encode('utf-8', errors='ignore'))
    return m.hexdigest()

# comment regex will be parsed and subbed in the other of definition
comments_regex = {
    # C style comments are also used in Java, JavaScript
    'c': [
        # Single / Multiline comments
        r'\/\*[\s\S]*?\*\/|([^:]|^)\/\/.*',
        r'\/\*[\s\S]*?$|^[\s\S]*?\*\/'  # Partial comments
    ],
    # perl also uses Python style comments
    'python': [
        r'#.*'  # Single line comment
        # For python, removal of docstrings is a challenge
        # since the hunk may have partial docstrings and we
        # cannot easily tell the start of the docstring from the end
    ],
    # php is a combination of C and Python comment styles
    'php': [
        # Single / Multiline comments
        r'\/\*[\s\S]*?\*\/|([^:]|^)\/\/.*|#.*',
        r'\/\*[\s\S]*?$|^[\s\S]*?\*\/'  # Partial comments
    ]
}

def skip_invalid_target_file(target_file):
    if 'test' in target_file.lower():
        return True
    words = ['man', 'doc', 'docs', 'readme', 'changelog', 'changelogs', 'news',
                'example', 'examples', 'testcase', 'testcases', 'tests']
    dir_list = [p.lower() for p in target_file.split('/')]
    # File name is at dir_list[-1].split('.')[0]
    dir_list[-1] = dir_list[-1].split('.')[0]
    return len(set(words).intersection(set(dir_list))) > 0

def create_hunk_record( hunk, target_file):
    langUtil = LanguageDetector()
    def get_hunk_code(lines):
        ll = []
        for l in lines:
            content = l.value.strip()
            if not re.sub(r'^(\*|-|#|\s)*$', '', content):
                continue
            ll.append(l.value)
        if len(ll) > 0:
            return '\n'.join(ll)
        return ''

    # Get language
    language = langUtil.detect_language(target_file)

    # Get Vul code, Patch code and hashes
    vul_code = strip_comments(get_hunk_code(hunk.source_lines()), language)
    patch_code = strip_comments(
        get_hunk_code(hunk.target_lines()), language)
    hunk_code = str(hunk)
    # Strip spacing in vulnerable_code and patch code for more
    # accurate duplicate detection with hash
    vul_code_hash = get_md5_hash(vul_code, strip_spacing=True)
    patch_code_hash = get_md5_hash(patch_code, strip_spacing=True)
    hunk_hash = get_md5_hash(hunk_code)

    # If vul code and patch code hashes are the same, discard
    if vul_code_hash == patch_code_hash:
        return None

    # Using ~~\~~x00 in the place of \x00 in the code, to insert into db
    return {
        'vulnerable_code': vul_code.replace('\x00', r'~~\~~x00'), 
        'patched_code': patch_code.replace('\x00', r'~~\~~x00'), 
        'hunk_code': hunk_code.replace('\x00', r'~~\~~x00'), 
        'affected_file': target_file.split()[0], 
        # 'affected_function': '',
        # 'extra_data': {'language': language} if language else {},
        # 'hunk_hash': hunk_hash,
        # 'is_valid': True,
        # 'vul_code_hash': vul_code_hash,
        # 'patch_code_hash': patch_code_hash,
        'hunk_id': next(hund_counter)
    }

def get_patch_date(patch_file):
    with open(patch_file, 'r') as file:
        patch_content = file.read()
    date_match = re.search(r"Date: (.+)", patch_content)
    if date_match:
        date_string = date_match.group(1)
        # datestring_formats = ['%a, %d %b %Y %H:%M:%S %z', '%a %b %d %H:%M:%S %Y %z', '%a, %d %b %Y %H:%M:%S %z %z']
        # for datestring_format in datestring_formats:
        #     try:
        #         date_obj = datetime.strptime(date_string.strip(), datestring_format)
        #         break
        #     except ValueError:
        #         pass
        # else:
        #     print(f'{patch_file} has invalid date format: {date_string}')
        #     return None
        # # 将 datetime 对象转换为指定格式的字符串
        try:
            date_obj = parser.parse(date_string)
            formatted_date = date_obj.strftime("%Y-%m-%d %H:%M:%S")
        except ValueError:
            try:
                # date_obj = parser.parse(' '.join(date_string.split(' ')[:-1]))
                date_obj = parser.parse(' '.join(date_string.replace('\t', ' ').split(' ')[:6]))
                formatted_date = date_obj.strftime("%Y-%m-%d %H:%M:%S")
            except ValueError:
                print(f'{patch_file} has invalid date format: {date_string}')
                return None

        # print("补丁的发布日期：", formatted_date)
        return formatted_date
    else:
        # print("未找到日期信息")
        return None


def get_patch_msg(patch_file):
    try:
        unidiff_patch_list = PatchSet.from_filename(patch_file)
    except Exception as e:
        if 'Unexpected trailing newline character' in str(e):
            with open(patch_file, 'r') as file:
                patch_content = file.read()
            patch_content = patch_content.rstrip('\n')
            unidiff_patch_list = PatchSet(patch_content)
        else:
            return []
    # release_date = first_patch.header_date
    # print(release_date)
    hunk_list = []
    if unidiff_patch_list:
        for p in unidiff_patch_list:  # type: PatchedFile
            # print(f"'--------\n' {p}")
            patch_unique_keys = []  # hunk_hash:affected_file
            # print(f"'--------\n' {p.target_file}")
            # if skip_invalid_target_file(p.target_file):
            #     continue
            for hunk in p:  # type: Hunk
                # Try creating a hunk record
                h = create_hunk_record(hunk, p.target_file)
                # print(f"'++++++++++\n' {h}")
                hunk_list.append(h)
    return hunk_list

def parse_cve_id(files_list):
    # extract list of cve_ids from old patch files list
    cve_id_list = files_list.split("_")[0]
    return cve_id_list

def patch_file_getter():
    # get patch from local files, in this case is in location 'downloads'
    # returns list of old patch files
    try: 
        files = os.listdir(PATCH_DIRECTORY)
        files = [file for file in files if os.path.isfile(os.path.join(PATCH_DIRECTORY, file))]
        return files
    except OSError as e:
        print(f"Error reading old patch directory: {e}")
        return""


patch_data = []
patch_files = patch_file_getter()
if __name__ == "__main__":
    for patch_file in patch_files:
        print('########' + patch_file)
        try:
            patch_msgs = get_patch_msg(PATCH_DIRECTORY + '/' + patch_file)
            patch_date = get_patch_date(PATCH_DIRECTORY + '/' + patch_file)
            
            if len(patch_msgs) == 0:
                with open('patch_file_skip.txt', 'a') as f:
                    f.write(patch_file +  '\n')
            patch_id = next(patch_counter)
            cve_id = parse_cve_id(patch_file)
            for i in range(len(patch_msgs)):
                if patch_msgs[i] is None:
                    # print('i will skip ')
                    continue
                else:
                    patch_data.append({
                            "id": patch_msgs[i]["hunk_id"],  
                            "public_id": cve_id,
                            "vulnerable_code": patch_msgs[i]["vulnerable_code"],
                            "patched_code": patch_msgs[i]["patched_code"],
                            "hunk_code": patch_msgs[i]["hunk_code"],
                            "affected_file": patch_msgs[i]["affected_file"],
                            "patch_id": patch_id,
                            "patch_date": patch_date,
                            "patch_file": patch_file
                        }
                    )
        except Exception as e:
            print(e)
            with open('patch_msg_error.txt', 'a') as f:
                f.write(patch_file + str(e) + '\n')
        
    with open('patch_msg.json', 'w') as f:
        f.write(json.dumps(patch_data))

    print('Parse done.')
    #print(hunk_list)