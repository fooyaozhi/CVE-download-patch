# from time import localtime, strftime
import multiprocessing
# import sys
import os
# import re
import requests
import hashlib
import datetime
import pytz
import json
import time
import random

from lib.Config import Configuration
# from lib.DatabaseLayer import PostgresDb
from lib.Logging import Logger

# import utils.database.postgresdb.DatabaseUpdater as DbUtil


# DEBUG = False
DEBUG, VERBOSE = Configuration.getStdoutSettings()

runPath = os.path.dirname(os.path.realpath(__file__))

logging_util = Logger(log_to_console=False, log_to_file=True)

# TODO handle if token run out of quota

# tokens are derived from everyone from scantist, please don't leak them out
# Use token_sum(20) for github crawling
try:
    token_sum = Configuration.getGITHUB().split('|')
    # token_sum = os.environ.get('GITHUB').split('|')
except:
    token_sum = []
    print('No valid token has been found')
QUOTA = 1000


def get_requests(url, token=None):
    """
    requests wrapped with tokens and check limits
    """
    status = True
    # while True:
    try:
        if token:
            res = requests.get(url, headers={'Authorization': f'token {token}'})
        else:
            res = requests.get(url)
    except Exception as e:
        raise Exception('get_requests', e)

    if 'X-RateLimit-Remaining' in res.headers:
        rate_limit_remaining = int(res.headers["X-RateLimit-Remaining"])
        if rate_limit_remaining < 200:
            print(token, 'invalid now')
            status = False
        # break
    elif 'RateLimit-Remaining' in res.headers:
        rate_limit_remaining = int(res.headers["RateLimit-Remaining"])
        if rate_limit_remaining < 10:
            print(token, 'invalid now')
            status = False
    return res.text, status


def get_available_token():
    return list(filter(check_remaining_limits, token_sum))


def check_remaining_limits(token):
    """
    check how many limits are allowed in this hour, foreach account, github only allows 5K times requests per hour
    """

    url = 'https://api.github.com/rate_limit'
    headers = {'Authorization': f'token {token}'}
    try:
        res = requests.get(url, headers=headers)
        if res.status_code == 401 or res.status_code == 403:
            return False
        res.raise_for_status()
    except Exception as e:
        print(e)
    remaining = json.loads(res.content.decode())['rate']['remaining']
    print(token, remaining)
    if remaining < QUOTA:
        return False
    else:
        return True


def md5_hash(data):
    m = hashlib.md5()
    m.update(data.encode())
    return m.hexdigest()


def isPatchFile(cnt):
    bad = ["<!", "<?"]
    keyword = ["insertions(+)", "deletions(-)", "diff --", "\n@@ "]
    raw = cnt.replace(" ", "").replace("\t", "").replace("\n", "")
    for i in bad:
        if raw.startswith(i):
            return 0
    for i in keyword:
        if i in cnt:
            return 1
    return 0


def downloadHTML(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'}
    cnt = requests.get(url, headers=headers, timeout=120).text
    return cnt


class PatchLink:
    def __init__(self, a, lock):
        tmp = a.split("\t")
        self.cve_id = tmp[0]
        self.patch_count = 0
        self.patch_hash = "N/A"
        self.url = tmp[1]
        self.error = 0
        self.lock = lock
        self.possible_repo = "N/A"
        #The record in patch log file is supposed to consist of "cve\tpatch_link\tconfidence".
        #Extract confidence if we can find it, if not, set confidence to be -1(dummy value).
        if len(tmp) == 3 and tmp[2].isdigit():
            self.confidence = tmp[2]
        else:
            self.confidence = -1

    def get_patch_info(self):
        if self.patch_count == 0:
            return 0

        return (self.cve_id, self.patch_count - 1, self.patch_hash)

    def get_error_info(self):
        if self.error == 0:
            return 0

        return (self.cve_id, self.url, self.error)

    def detect_repo(self):
        if "//github.com/" in self.url or "//gitlab.com/" in self.url:
            if "/commit/" in self.url:
                self.possible_repo = self.url[:self.url.index("/commit/")]
        elif ".git" in self.url or "/gitweb/" in self.url:
            if "h=" in self.url and ";" in self.url:
                self.possible_repo = self.url[:self.url.index(";")]
            elif "/patch/" in self.url:
                self.possible_repo = self.url[:self.url.index("/patch/")]
        elif "/+/" in self.url:
            self.possible_repo = self.url[:self.url.index("/+/")]
        elif "/raw-rev/" in self.url:
            self.possible_repo = self.url[:self.url.index("/raw-rev/")]
        elif "/diff/" in self.url:
            self.possible_repo = self.url[:self.url.index("/diff/")]
        elif "/commits/" in self.url:
            self.possible_repo = self.url[:self.url.index("/commits/")]
        return self.cve_id, self.possible_repo

    def download_patch(self, cve_id_dict, existing_patch_cve, available_tokens):
        module_name = __name__ + '.' + self.download_patch.__name__

        try:
            if 'github.com' in self.url:
                if available_tokens:
                    for token in available_tokens:
                        cnt, status = get_requests(self.url, token)
                        print('downloading github patch using token %s'%token)
                        if status:
                            break
                        else:
                            continue
                    if not status:
                        # quota run out for all tokens, waiting for refilling
                        time.sleep(60 * 10)
                        # random choose a token after refilling
                        random.seed(datetime.now())
                        token = random.choice(available_tokens)
                        cnt, status = get_requests(self.url, token)
                else:
                    cnt = downloadHTML(self.url)
            elif 'PREPATCH' in self.url:
                print('@@@@ prepatch is found :', self.url)
                with open(f"{PRE_PATCH_DIR}/{self.url.lstrip('PREPATCH')}", 'r') as f:
                    cnt = f.read()
            else:
                cnt = downloadHTML(self.url)
            if "googlesource" in self.url:
                try:
                    cnt = cnt.decode("base64")
                except:
                    pass

        except Exception as e:
            self.error = str(e)
            return 0

        if isPatchFile(cnt) == 0:
            self.error = "Not A Patch File"
            return 0

        # year = self.cve_id[4:8]
        # if os.path.exists(os.path.join(PATCH_DIR, year)) is False:
        #     os.makedirs(os.path.join(PATCH_DIR, year))
        if os.path.exists(PATCH_DIR) is False:
            os.makedirs(PATCH_DIR)

        # path = PATCH_DIR + "/" + year + "/" + self.cve_id
        path = PATCH_DIR + "/" + self.cve_id
        patch_routine = ROUTINE_DIR + "/" + self.cve_id

        try:

            self.lock.acquire()
            c = 0
            while os.path.exists("%s_%d.patch" % (path, c)):
                c += 1
            self.patch_count = c + 1
            self.patch_hash = md5_hash(cnt)
            # if self.cve_id in cve_id_dict and (cve_id_dict[self.cve_id], self.patch_hash) in existing_patch_cve:
            #     # Not Success or Fail
            #     return 2
            # else:
            if True: ###
                if (self.cve_id, self.patch_hash) not in EXISTING_PATCHES:
                    open("%s_%d.patch" % (path, c), "wb").write(cnt.encode())
                    open("%s_%d.patch" %
                         (patch_routine, c), "wb").write(cnt.encode())
                else:
                    logging_util.log(module_name,
                                     "[+] patch file is already extracted: %s - %s" % (self.cve_id, str(self.patch_hash)))

        except Exception as e:
            self.error = str(e)
            return 0
        finally:
            self.lock.release()

        return 1


def worker(patch_queue, cve_id_dict, existing_patch_cve, lock, available_tokens):
    module_name = __name__ + '.' + worker.__name__

    while 1:
        tmp = patch_queue.get()
        if tmp == "STOP":
            break

        if "\t" in tmp:
            cve = PatchLink(tmp, lock)
            (id, repo) = cve.detect_repo()
            result = cve.download_patch(cve_id_dict, existing_patch_cve, available_tokens)

            try:
                lock.acquire()
                last_update = open(os.path.join(
                    LOG_DIR, "History"), "r").read().splitlines()[-1]
                repo_logfile = os.path.join(
                    LOG_DIR, last_update.replace("FOUND", "REPO"))
                open(repo_logfile, "a").write("%s\t%s\n" % (id, repo))

                if result == 0:
                    error_info = cve.get_error_info()
                    url = ''
                    if len(error_info) > 2:
                        url = error_info[1]
                    logging_util.log(module_name, error_info, level='error')
                    with open(os.path.join(LOG_DIR, last_update.replace('FOUND', 'NOT_FOUND')), 'a') as f:
                        f.write("%s\t%s\t%s\n" % (id, url, cve.confidence))

                elif result == 1:
                    # Successfully downloaded patch
                    with open(os.path.join(LOG_DIR, last_update.replace('FOUND', 'SUCCESS')), 'a') as f:
                        f.write("%s\t%s\t%s\t%s\n" % (id, cve.url, cve.confidence, cve.patch_hash))

            except Exception as e:
                print(tmp, e)
            finally:
                lock.release()
    return 1


def create_patches_dict(cve_id_dict, existing_patch_cve):
    patches_dict = {}

    for root, _dirs, files in os.walk(PATCH_DIR):
        for names in files:
            filepath = os.path.join(root, names)
            m = hashlib.md5()
            with open(filepath, 'rb') as afile:
                buf = afile.read()
                m.update(buf)
                if names.count('_') > 1:
                    parts = names.split('_')
                    parts.pop()
                    cve_id = '-'.join(parts)
                else:
                    cve_id = names.split('_')[0]
                try:
                    if cve_id in cve_id_dict and (cve_id_dict[cve_id], m.hexdigest()) not in existing_patch_cve:
                        # Postgres bytea fields have an implicit 1 GB limit
                        # Anyway, prevent writing patches > 50 MB data
                        if len(buf) < 52428800:
                            patches_dict[cve_id + '_' + m.hexdigest()] = {
                                'cve_id': cve_id,
                                'raw': buf,
                                'hash': m.hexdigest()
                            }
                        else:
                            print (f'Error - Patch larger than 10 MB. Size: {len(buf)}. Skipping.')
                except Exception as e:
                    logging_util.log('encoding error', str(e))
    return patches_dict


def get_patch_url_status_dict():
    last_update = open(os.path.join(LOG_DIR, 'History'),
                       'r').read().splitlines()[-1]
    all_logfile = os.path.join(LOG_DIR, last_update)
    not_found_logfile = all_logfile.replace('FOUND', 'NOT_FOUND')
    downloaded_logfile = all_logfile.replace('FOUND', 'SUCCESS')
    all_urls = set()  # All urls found
    found_urls = set()  # Urls where a patch was available
    not_found_urls = set()  # Invalid urls
    success_urls = set()  # Urls from which a patch was downloaded

    # Get all urls
    with open(all_logfile, 'r') as f:
        all_urls = f.read().splitlines()
    all_urls = set(all_urls)

    # Get urls from which patch could not be extracted
    if os.path.exists(not_found_logfile):
        with open(not_found_logfile, 'r') as f:
            not_found_urls = f.read().splitlines()
        not_found_urls = set(not_found_urls)

    # Get urls from which the patch was successfully downloaded
    if os.path.exists(downloaded_logfile):
        with open(downloaded_logfile, 'r') as f:
            success_urls = f.read().splitlines()
        success_urls = set(success_urls)

    success_url_mapped = set(map(lambda x: ('\t'.join(x.split('\t')[:3])), success_urls))

    # Get urls from which patch could be extracted but patch was not downloaded
    # (Eg: Duplicated patch hash)
    found_urls = all_urls.difference(not_found_urls)
    found_urls = found_urls.difference(success_url_mapped)

    found_list = [tuple(l.split('\t')) for l in found_urls]
    not_found_list = [tuple(l.split('\t')) for l in not_found_urls]
    success_list = [tuple(l.split('\t')) for l in success_urls]

    patch_source_data = []
    for l in found_list:
        l = l + (True, )
        patch_source_data.append(l)
    for l in not_found_list:
        l = l + (False, )
        patch_source_data.append(l)

    # Include success list as it is - cve_id, url, patch_hash
    patch_source_data = success_list + patch_source_data

    return patch_source_data


LOG_DIR = os.path.join(runPath, "Log")
PRE_PATCH_DIR = os.path.join(runPath, 'Log', 'pre_patch_files')
PATCH_DIR = os.path.join(runPath, 'srj-patch_files', 'patch')

extraction_time = datetime.datetime.now(pytz.utc).strftime('%Y-%m-%d_%H-%M')
ROUTINE_DIR = os.path.join(runPath, 'srj-patch_files',
                           'patch_routine', extraction_time)
EXISTING_PATCHES = []  # (cve_pub_id, patch_hash)

for root, dirs, files in os.walk(PATCH_DIR):
    for names in files:
        filepath = os.path.join(root, names)
        m = hashlib.md5()
        with open(filepath, 'rb') as afile:
            buf = afile.read()
            m.update(buf)
        cve_id = names.split('_')[0]
        EXISTING_PATCHES.append((cve_id, m.hexdigest()))

if __name__ == "__main__":
    module_name = __name__ + '.' + '__main__'
    logging_util.log(module_name, "Begin Download ...")
    print("Begin Download")

    # # init database
    # postgresdb = PostgresDb(*Configuration.getPostgresConnection())

    # default input list
    last_update = open(os.path.join(LOG_DIR, "History"),
                       "r").read().splitlines()[-1]
    patch_list = open(os.path.join(LOG_DIR, last_update),
                      'r').read().splitlines()

    if not os.path.isdir(ROUTINE_DIR):
        os.makedirs(ROUTINE_DIR)

    if not os.path.isdir(PATCH_DIR):
        os.makedirs(PATCH_DIR)

    # status variables
    process_num = multiprocessing.cpu_count() * 2
    patch_queue = multiprocessing.Queue()
    proces_list = []

    for i in patch_list:
        patch_queue.put(i)

    # Create a lock for writing to shared dict
    lock = multiprocessing.Lock()

    # Get existing patch dict and CVE dict
    # existing_patch_cve = postgresdb.get_patch_hash_cve_list()
    # cve_id_dict = postgresdb.get_cve_id_dict()
    existing_patch_cve = []
    cve_id_dict = {}

    available_tokens = get_available_token()
    # spawn multiprocesses
    for i in range(process_num):
        p = multiprocessing.Process(target=worker,
                                    args=(patch_queue, cve_id_dict, existing_patch_cve, lock, available_tokens))
        proces_list.append(p)
        patch_queue.put("STOP")
        p.start()

    for p in proces_list:
        p.join()

    # # Create a dict of patches to insert
    # patches_dict = create_patches_dict(cve_id_dict, existing_patch_cve)
    # # Insert patches into postgres db
    # cve_list = [line.rstrip('\n') for line in open(
    #     os.path.join(LOG_DIR, 'finalResult'))]
    # DbUtil.insert_patches(postgresdb, patches_dict, cve_list)
    # # Update Patch URL status in postgresdb
    # patch_source_data = get_patch_url_status_dict()
    # DbUtil.insert_patch_sources(
    #     postgresdb, patch_source_data, cve_id_dict, logging_util)

    # # For monitoring
    # last_update = open(os.path.join(LOG_DIR, 'History'),
    #                    'r').read().splitlines()[-1]
    # full_patch_link = open(os.path.join(LOG_DIR, last_update),
    #                        'r').read().splitlines()
    # not_found_logfile = last_update.replace('FOUND', 'NOT_FOUND')
    # if os.path.isfile(os.path.join(LOG_DIR, not_found_logfile)):
    #     not_found_patch_link = open(os.path.join(LOG_DIR, not_found_logfile),
    #                                 'r').read().splitlines()
    # else:
    #     not_found_patch_link = []
    # patch_monitor = {'input': len(full_patch_link),
    #                  'error': len(not_found_patch_link),
    #                  'output': len(full_patch_link) - len(not_found_patch_link)}

    # DbUtil.insert_patch_monitor(postgresdb, patch_monitor)

    logging_util.log(module_name, "Download Done ...")
    print("Download Done")