# from time import localtime, strftime
import multiprocessing
# import sys
import os
# import re
import requests
import hashlib
import datetime
import pytz
import json
import time
import random
import sqlite3

from lib.Config import Configuration
# from lib.DatabaseLayer import PostgresDb
from lib.Logging import Logger

# import utils.database.postgresdb.DatabaseUpdater as DbUtil


# DEBUG = False
DEBUG, VERBOSE = Configuration.getStdoutSettings()

runPath = os.path.dirname(os.path.realpath(__file__))

logging_util = Logger(log_to_console=False, log_to_file=True)

# TODO handle if token run out of quota

# tokens are derived from everyone from scantist, please don't leak them out
# Use token_sum(20) for github crawling
try:
    token_sum = Configuration.getGITHUB().split('|')
    # token_sum = os.environ.get('GITHUB').split('|')
except:
    token_sum = []
    print('No valid token has been found')
QUOTA = 1000


def get_requests(url, token=None):
    """
    requests wrapped with tokens and check limits
    """
    status = True
    # while True:
    try:
        if token:
            res = requests.get(url, headers={'Authorization': f'token {token}'})
        else:
            res = requests.get(url)
    except Exception as e:
        raise Exception('get_requests', e)

    if 'X-RateLimit-Remaining' in res.headers:
        rate_limit_remaining = int(res.headers["X-RateLimit-Remaining"])
        if rate_limit_remaining < 200:
            print(token, 'invalid now')
            status = False
        # break
    elif 'RateLimit-Remaining' in res.headers:
        rate_limit_remaining = int(res.headers["RateLimit-Remaining"])
        if rate_limit_remaining < 10:
            print(token, 'invalid now')
            status = False
    return res.text, status


def get_available_token():
    return list(filter(check_remaining_limits, token_sum))


def check_remaining_limits(token):
    """
    check how many limits are allowed in this hour, foreach account, github only allows 5K times requests per hour
    """

    url = 'https://api.github.com/rate_limit'
    headers = {'Authorization': f'token {token}'}
    try:
        res = requests.get(url, headers=headers)
        if res.status_code == 401 or res.status_code == 403:
            return False
        res.raise_for_status()
    except Exception as e:
        print(e)
    remaining = json.loads(res.content.decode())['rate']['remaining']
    print(token, remaining)
    if remaining < QUOTA:
        return False
    else:
        return True


def md5_hash(data):
    m = hashlib.md5()
    m.update(data.encode())
    return m.hexdigest()


def isPatchFile(cnt):
    bad = ["<!", "<?"]
    keyword = ["insertions(+)", "deletions(-)", "diff --", "\n@@ "]
    raw = cnt.replace(" ", "").replace("\t", "").replace("\n", "")
    for i in bad:
        if raw.startswith(i):
            return 0
    for i in keyword:
        if i in cnt:
            return 1
    return 0


def downloadHTML(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'}
    cnt = requests.get(url, headers=headers, timeout=120).text
    return cnt


class PatchLink:
    def __init__(self, a, lock):
        tmp = a.split("\t")
        self.cve_id = tmp[0]
        self.patch_count = 0
        self.patch_hash = "N/A"
        self.url = tmp[1]
        self.error = 0
        self.lock = lock
        self.possible_repo = "N/A"
        self.file_name = '???'
        #The record in patch log file is supposed to consist of "cve\tpatch_link\tconfidence".
        #Extract confidence if we can find it, if not, set confidence to be -1(dummy value).
        if len(tmp) == 4 and tmp[2].isdigit():
            self.confidence = tmp[2]
            self.patch_extract_source = tmp[3]
        else:
            self.confidence = -1
            self.patch_extract_source = tmp[-1]

    def get_patch_info(self):
        if self.patch_count == 0:
            return 0

        return (self.cve_id, self.patch_count - 1, self.patch_hash)

    def get_error_info(self):
        if self.error == 0:
            return 0

        return (self.cve_id, self.url, self.error)

    def detect_repo(self):
        if "//github.com/" in self.url or "//gitlab.com/" in self.url:
            if "/commit/" in self.url:
                self.possible_repo = self.url[:self.url.index("/commit/")]
        elif ".git" in self.url or "/gitweb/" in self.url:
            if "h=" in self.url and ";" in self.url:
                self.possible_repo = self.url[:self.url.index(";")]
            elif "/patch/" in self.url:
                self.possible_repo = self.url[:self.url.index("/patch/")]
        elif "/+/" in self.url:
            self.possible_repo = self.url[:self.url.index("/+/")]
        elif "/raw-rev/" in self.url:
            self.possible_repo = self.url[:self.url.index("/raw-rev/")]
        elif "/diff/" in self.url:
            self.possible_repo = self.url[:self.url.index("/diff/")]
        elif "/commits/" in self.url:
            self.possible_repo = self.url[:self.url.index("/commits/")]
        return self.cve_id, self.possible_repo

    def download_patch(self, cve_id_dict, existing_patch_cve, available_tokens):
        module_name = __name__ + '.' + self.download_patch.__name__

        try:
            if 'github.com' in self.url:
                if available_tokens:
                    for token in available_tokens:
                        cnt, status = get_requests(self.url, token)
                        print('downloading github patch using token %s'%token)
                        if status:
                            break
                        else:
                            continue
                    if not status:
                        # quota run out for all tokens, waiting for refilling
                        time.sleep(60 * 10)
                        # random choose a token after refilling
                        random.seed(datetime.now())
                        token = random.choice(available_tokens)
                        cnt, status = get_requests(self.url, token)
                else:
                    cnt = downloadHTML(self.url)
            elif 'PREPATCH' in self.url:
                print('@@@@ prepatch is found :', self.url)
                with open(f"{PRE_PATCH_DIR}/{self.url.lstrip('PREPATCH')}", 'r') as f:
                    cnt = f.read()
            else:
                cnt = downloadHTML(self.url)
            if "googlesource" in self.url:
                try:
                    cnt = cnt.decode("base64")
                except:
                    pass

        except Exception as e:
            self.error = str(e)
            return 0

        if isPatchFile(cnt) == 0:
            self.error = "Not A Patch File"
            return 0

        # year = self.cve_id[4:8]
        # if os.path.exists(os.path.join(PATCH_DIR, year)) is False:
        #     os.makedirs(os.path.join(PATCH_DIR, year))
        if os.path.exists(PATCH_DIR) is False:
            os.makedirs(PATCH_DIR)

        # path = PATCH_DIR + "/" + year + "/" + self.cve_id
        path = PATCH_DIR + "/" + self.cve_id
        patch_routine = ROUTINE_DIR + "/" + self.cve_id

        try:

            self.lock.acquire()
            c = 0
            while os.path.exists("%s_%d.patch" % (path, c)):
                c += 1
            self.patch_count = c + 1
            self.patch_hash = md5_hash(cnt)
            # if self.cve_id in cve_id_dict and (cve_id_dict[self.cve_id], self.patch_hash) in existing_patch_cve:
            #     # Not Success or Fail
            #     return 2
            # else:
            if (self.cve_id, self.patch_hash) not in EXISTING_PATCHES:
                open("%s_%d.patch" % (path, c), "wb").write(cnt.encode())
                self.file_name = f"{self.cve_id}_{c}.patch"
                # open("%s_%d.patch" %
                #      (patch_routine, c), "wb").write(cnt.encode())
            else:
                logging_util.log(module_name,
                                    "[+] patch file is already extracted: %s - %s" % (self.cve_id, str(self.patch_hash)))
                self.file_name = self.patch_hash

        except Exception as e:
            self.error = str(e)
            return 0
        finally:
            self.lock.release()
            self.file_name = f"{self.cve_id}_{c}.patch"
        return 1


def worker(patch_queue, cve_id_dict, existing_patch_cve, lock, available_tokens):
    module_name = __name__ + '.' + worker.__name__

    while 1:
        tmp = patch_queue.get()
        if tmp == "STOP":
            break

        if "\t" in tmp:
            cve = PatchLink(tmp, lock)
            (id, repo) = cve.detect_repo()
            result = cve.download_patch(cve_id_dict, existing_patch_cve, available_tokens)

            try:
                lock.acquire()
                last_update = open(os.path.join(
                    LOG_DIR, "History"), "r").read().splitlines()[-1]
                repo_logfile = os.path.join(
                    LOG_DIR, last_update.replace("FOUND", "REPO"))
                open(repo_logfile, "a").write("%s\t%s\n" % (id, repo))

                if result == 0:
                    error_info = cve.get_error_info()
                    url = ''
                    if len(error_info) > 2:
                        url = error_info[1]
                    logging_util.log(module_name, error_info, level='error')
                    with open(os.path.join(LOG_DIR, last_update.replace('FOUND', 'NOT_FOUND')), 'a') as f:
                        f.write("%s\t%s\t%s\t%s\n" % (id, url, cve.confidence, cve.error))

                elif result == 1:
                    # Successfully downloaded patch
                    with open(os.path.join(LOG_DIR, last_update.replace('FOUND', 'SUCCESS')), 'a') as f:
                        # f.write("%s\t%s\t%s\t%s\n" % (id, cve.url, cve.confidence, cve.patch_hash))
                        f.write("%s\t%s\t%s\t%s\n" % (id, cve.url, cve.file_name, cve.patch_hash))

            except Exception as e:
                print(tmp, e)
            finally:
                lock.release()
    return 1


# def create_patches_dict(cve_id_dict, existing_patch_cve):
#     patches_dict = {}

#     for root, _dirs, files in os.walk(PATCH_DIR):
#         for names in files:
#             filepath = os.path.join(root, names)
#             m = hashlib.md5()
#             with open(filepath, 'rb') as afile:
#                 buf = afile.read()
#                 m.update(buf)
#                 if names.count('_') > 1:
#                     parts = names.split('_')
#                     parts.pop()
#                     cve_id = '-'.join(parts)
#                 else:
#                     cve_id = names.split('_')[0]
#                 try:
#                     if cve_id in cve_id_dict and (cve_id_dict[cve_id], m.hexdigest()) not in existing_patch_cve:
#                         # Postgres bytea fields have an implicit 1 GB limit
#                         # Anyway, prevent writing patches > 50 MB data
#                         if len(buf) < 52428800:
#                             patches_dict[cve_id + '_' + m.hexdigest()] = {
#                                 'cve_id': cve_id,
#                                 'raw': buf,
#                                 'hash': m.hexdigest()
#                             }
#                         else:
#                             print (f'Error - Patch larger than 10 MB. Size: {len(buf)}. Skipping.')
#                 except Exception as e:
#                     logging_util.log('encoding error', str(e))
#     return patches_dict


# def get_patch_url_status_dict():
#     last_update = open(os.path.join(LOG_DIR, 'History'),
#                        'r').read().splitlines()[-1]
#     all_logfile = os.path.join(LOG_DIR, last_update)
#     not_found_logfile = all_logfile.replace('FOUND', 'NOT_FOUND')
#     downloaded_logfile = all_logfile.replace('FOUND', 'SUCCESS')
#     all_urls = set()  # All urls found
#     found_urls = set()  # Urls where a patch was available
#     not_found_urls = set()  # Invalid urls
#     success_urls = set()  # Urls from which a patch was downloaded

#     # Get all urls
#     with open(all_logfile, 'r') as f:
#         all_urls = f.read().splitlines()
#     all_urls = set(all_urls)

#     # Get urls from which patch could not be extracted
#     if os.path.exists(not_found_logfile):
#         with open(not_found_logfile, 'r') as f:
#             not_found_urls = f.read().splitlines()
#         not_found_urls = set(not_found_urls)

#     # Get urls from which the patch was successfully downloaded
#     if os.path.exists(downloaded_logfile):
#         with open(downloaded_logfile, 'r') as f:
#             success_urls = f.read().splitlines()
#         success_urls = set(success_urls)

#     success_url_mapped = set(map(lambda x: ('\t'.join(x.split('\t')[:3])), success_urls))

#     # Get urls from which patch could be extracted but patch was not downloaded
#     # (Eg: Duplicated patch hash)
#     found_urls = all_urls.difference(not_found_urls)
#     found_urls = found_urls.difference(success_url_mapped)

#     found_list = [tuple(l.split('\t')) for l in found_urls]
#     not_found_list = [tuple(l.split('\t')) for l in not_found_urls]
#     success_list = [tuple(l.split('\t')) for l in success_urls]

#     patch_source_data = []
#     for l in found_list:
#         l = l + (True, )
#         patch_source_data.append(l)
#     for l in not_found_list:
#         l = l + (False, )
#         patch_source_data.append(l)

#     # Include success list as it is - cve_id, url, patch_hash
#     patch_source_data = success_list + patch_source_data

#     return patch_source_data


LOG_DIR = os.path.join(runPath, "Log")
PRE_PATCH_DIR = os.path.join(runPath, 'Log', 'pre_patch_files')
PATCH_DIR = os.path.join(runPath, 'srj-patch_files', 'patch')

extraction_time = datetime.datetime.now(pytz.utc).strftime('%Y-%m-%d_%H-%M')
ROUTINE_DIR = os.path.join(runPath, 'srj-patch_files',
                           'patch_routine', extraction_time)
EXISTING_PATCHES = []  # (cve_pub_id, patch_hash)

for root, dirs, files in os.walk(PATCH_DIR):
    for names in files:
        filepath = os.path.join(root, names)
        m = hashlib.md5()
        with open(filepath, 'rb') as afile:
            buf = afile.read()
            m.update(buf)
        cve_id = names.split('_')[0]
        EXISTING_PATCHES.append((cve_id, m.hexdigest()))

if __name__ == "__main__":
    module_name = __name__ + '.' + '__main__'
    logging_util.log(module_name, "Begin Download ...")
    print("Begin Download")

    # # init database
    # postgresdb = PostgresDb(*Configuration.getPostgresConnection())

    # default input list
    last_update = open(os.path.join(LOG_DIR, "History"),
                       "r").read().splitlines()[-1]
    patch_list = open(os.path.join(LOG_DIR, last_update),
                      'r').read().splitlines()
    
    print(f'Patch list file to download url: {last_update}')
    patch_list_nonsourceware = [_ for _ in patch_list if 'sourceware' not in _]
    patch_list_sourceware = [_ for _ in patch_list if 'sourceware' in _]
    print(len(patch_list_nonsourceware), len(patch_list_sourceware))

    if not os.path.isdir(ROUTINE_DIR):
        os.makedirs(ROUTINE_DIR)

    if not os.path.isdir(PATCH_DIR):
        os.makedirs(PATCH_DIR)

    # status variables
    process_num = multiprocessing.cpu_count() * 2
    patch_queue = multiprocessing.Queue()
    proces_list = []

    # for i in patch_list:
    for i in patch_list_nonsourceware:
        patch_queue.put(i)

    # Create a lock for writing to shared dict
    lock = multiprocessing.Lock()

    # Get existing patch dict and CVE dict
    # existing_patch_cve = postgresdb.get_patch_hash_cve_list()
    # cve_id_dict = postgresdb.get_cve_id_dict()
    existing_patch_cve = []
    cve_id_dict = {}

    # 1. Download non-sourceware patches #
    available_tokens = get_available_token()
    # spawn multiprocesses
    for i in range(process_num):
        p = multiprocessing.Process(target=worker,
                                    args=(patch_queue, cve_id_dict, existing_patch_cve, lock, available_tokens))
        proces_list.append(p)
        patch_queue.put("STOP")
        p.start()

    for p in proces_list:
        p.join()

    # 2. Download sourceware patches #
    print('Commencing sourceware patch download ...')
    # read from sqlite3, because high chance of failure hence need to ignore processed ones
    def update_sqlite_cve_patch_status(cursor, statement):
        try:
            cursor.execute(statement)
        except Exception as e:
            print(f'Error: {e}')
            conn.rollback()
        else:
            conn.commit()

    # Create a connection to the database
    conn = sqlite3.connect('test.db')
    cur = conn.cursor()   
    sourceware_patch_list = cur.execute('SELECT * FROM cve_patch_status where patch_link like "%sourceware%" and status is Null').fetchall()
    print('Number of sourceware links to download: ', len(sourceware_patch_list)) 

    ####
    current_counter = 0
    for entry in sourceware_patch_list:
        current_counter += 1
        cve = entry[0]
        patchlink = entry[1]
        path = PATCH_DIR + "/" + cve
        time.sleep(3)
        # start download
        c = 0
        original_link = patchlink
        patchlink = patchlink.replace('a=commitdiff','a=patch') 
        patchlink = patchlink.replace('a=commit','a=patch') 
        patchlink = patchlink.replace('a=blobdiff','a=patch') 
        if 'bugzilla' in patchlink:
            patchlink = patchlink.replace('&amp;action=diff','')

        try:
            cnt = downloadHTML(patchlink)
        except Exception as e:
            print(f'[{datetime.datetime.now()}] Request Error for {cve} {patchlink}: \n{e}')
            cur.close()
            conn.close()
            exit(1)
        for errormsg in ['The requested URL was not found on this server','404 - Unknown commit object']:
            if errormsg in cnt:
                # Valid error
                print(f'{current_counter:} Error: VALID ERROR {cve} {patchlink} {errormsg}')
                update_sqlite_cve_patch_status(cur, f'UPDATE cve_patch_status SET status = "Failure: {errormsg}" WHERE cve = "{cve}" and patch_link = "{original_link}"')
                continue
        if isPatchFile(cnt) == 0:
                print(f'{current_counter:} Error: NOT PATCH FILE {cve} {patchlink}')
                update_sqlite_cve_patch_status(cur, f'UPDATE cve_patch_status SET status = "Failure: Not a patch file." WHERE cve = "{cve}" and patch_link = "{original_link}"')
        patch_hash = md5_hash(cnt)
        if (cve, patch_hash) not in EXISTING_PATCHES:
            while os.path.exists(f'{path}_{c}.patch'):
                c += 1
            patch_file_name = f"{path}_{c}.patch"
            open(patch_file_name, "wb").write(cnt.encode())
            print(f'{current_counter:} {cve} {patchlink} Downloaded to {patch_file_name}')
            update_sqlite_cve_patch_status(cur, f'UPDATE cve_patch_status SET status = "Success", file_name = "{cve}_{c}.patch" WHERE cve = "{cve}" and patch_link = "{original_link}"')
        else:
            print(f'{current_counter:} {cve} {patchlink} Already exist as {patch_hash}.')
            update_sqlite_cve_patch_status(cur, f'UPDATE cve_patch_status SET status = "Success, already exist.", file_name = "{patch_hash}" WHERE cve = "{cve}" and patch_link = "{original_link}"')

    cur.close()
    conn.close()

    # # Create a dict of patches to insert
    # patches_dict = create_patches_dict(cve_id_dict, existing_patch_cve)
    # # Insert patches into postgres db
    # cve_list = [line.rstrip('\n') for line in open(
    #     os.path.join(LOG_DIR, 'finalResult'))]
    # DbUtil.insert_patches(postgresdb, patches_dict, cve_list)
    # # Update Patch URL status in postgresdb
    # patch_source_data = get_patch_url_status_dict()
    # DbUtil.insert_patch_sources(
    #     postgresdb, patch_source_data, cve_id_dict, logging_util)

    # # For monitoring
    # last_update = open(os.path.join(LOG_DIR, 'History'),
    #                    'r').read().splitlines()[-1]
    # full_patch_link = open(os.path.join(LOG_DIR, last_update),
    #                        'r').read().splitlines()
    # not_found_logfile = last_update.replace('FOUND', 'NOT_FOUND')
    # if os.path.isfile(os.path.join(LOG_DIR, not_found_logfile)):
    #     not_found_patch_link = open(os.path.join(LOG_DIR, not_found_logfile),
    #                                 'r').read().splitlines()
    # else:
    #     not_found_patch_link = []
    # patch_monitor = {'input': len(full_patch_link),
    #                  'error': len(not_found_patch_link),
    #                  'output': len(full_patch_link) - len(not_found_patch_link)}

    # DbUtil.insert_patch_monitor(postgresdb, patch_monitor)

    logging_util.log(module_name, "Download Done ...")
    print("Download Done")